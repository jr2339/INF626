---
title: "Lab6"
author: "Jun Rao"
date: "10/1/2019"
output: word_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h')
```

# Model selection in R and Stan 

I have provided you with a data set in which you have measured outcome $y$ and you have measured two input variables $x1$ and $x2$. Your full, most complex linear model that we will consider is:
$$y_i = \alpha + \beta_1 X_{1,i} + \beta_2 X_{2,i}$$
Therefore, we will ignore any multiplicative interactions between the inputs. Importantly, remember from your previous statistics classes that we can condense linear regression into matrix notation: $y = \alpha + \mathbf{B} \mathbf{X}$, where $\mathbf{B}$ is a row-vector that holds the slopes and $\mathbf{X}$ is an input matrix of dimensions ($n \times d$), where $n$ is the number of data points and $d$ is the number of input variables. 

## Task 1 (15 points)
```{r, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE}
#--------------------------------------
#--------------------------------------
# You may need to install some of these packages
library(tidyverse)
library(rstan)
library(loo)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
#--------------------------------------
#--------------------------------------


```
1. Import your data (code provided below), and then center and scale your input variables. Center by the mean and scale by 2$\times$standard deviation. This assignment will not work unless you do this step properly. Hint: You must center and scale the two $x$ variable columns separately (i.e., each variable will have its own mean and st. deviation). 
```{r, fig.align='center', fig.height=4, fig.width=8, message=FALSE, warning=FALSE}

# Read in the .CSV file:
data_raw = read_csv("lab6_data.csv")
#observations
n_sample = nrow(data_raw)
n_sample
#y
y = data_raw$y

#x1
x1 = data_raw$x1
x1_mean = mean(x1)
x1_sd = sd(x1)
#X2
x2 = data_raw$x2
x2_mean = mean(x2)
x2_sd = sd(x2)

#center and standardize:
x1_scaled = (x1 - x1_mean)/(2*x1_sd)
x2_scaled = (x2 - x2_mean)/(2*x2_sd)
```

2. Create two scatterplots of your outcome versus your two inputs. 
```{r}
plot(y, x1_scaled, main="y ~ x1",xlab="x1", ylab="y", pch=19)
```
```{r}
plot(y, x2_scaled, main="y ~ x2",xlab="x2", ylab="y", pch=19)
```
3. Look at the provided Stan script, \texttt{Multi\_LinReg.stan}. Come to an understanding of the data inputs required to run this Stan model. Use the centered and scaled inputs to set up and run the Stan model, by adapting previously provided code. Hint: You will have to monitor the \texttt{log\_lik} parameter. 
```{r, message=FALSE, warning=FALSE}
stan_data = list(n_input = 2,
                 n_sample = n_sample,
                 x_mat = cbind(x1_scaled,x2_scaled),
                 y_vec = y)

params_monitor = c("beta", "alpha", "sigma", "log_lik")

test_fit = stan(file = "Multi_LinReg.stan",
                data = stan_data,
                pars = params_monitor,
                chains = 1, # How many chains to run
                iter = 10, # How many iterations per chain
                algorithm="NUTS")

## Now we will run our full model:
# How many samples do we want of each parameter, from each chain?
n_mc_samples = 1000
# How much burn-in?
n_burn = 500
# How much thinning? (take the ith value of the chain)
n_thin = 3
# Total iterations needed:
n_iter_total = (n_mc_samples * n_thin) + n_burn
model_fit =
 stan(fit = test_fit, # So it knows we're already compiled
      file = "Multi_LinReg.stan",
      data = stan_data,
      pars = params_monitor,
      chains = 3,
      warmup = n_burn,
      thin = n_thin,
      iter = n_iter_total,
       algorithm="NUTS")
model_out = rstan::extract(model_fit)
str(model_out)
```

4. Show me that your model has converged. 
```{r}
# Check Convergence:
tail(summary(model_fit)$summary[, "Rhat"],1)
```
Since the value of lp at here less than 1.1, hence we can conclude that the model is converged

5. What are the median and 95% credible intervals for each model parameter? 
```{r}
#the median
summary(model_fit)$summary[1:4, "50%"]
```


```{r}
#the 95% credible intervals
summary(model_fit)$summary[1:4, "2.5%"]
summary(model_fit)$summary[1:4, "97.5%"]
```


\newpage

## Task 2 (30 points)

1. Re-run your model two more times to store the nested model versions (e.g., with only input 1, and then with only input 2). Hint: You must store these models in objects with different names, however, you will not have to re-write the Stan model at all. Just think about how the inputs might change.
```{r, message=FALSE, warning=FALSE}
#Only input1
stan_data_1 = list(n_input = 1,
                 n_sample = n_sample,
                 x_mat = cbind(x1_scaled),
                 y_vec = y)
#Only input2
stan_data_2 = list(n_input = 1,
                 n_sample = n_sample,
                 x_mat = cbind(x2_scaled),
                 y_vec = y)

params_monitor = c("beta", "alpha", "sigma", "log_lik")

test_fit_1 = stan(file = "Multi_LinReg.stan",
                data = stan_data_1,
                pars = params_monitor,
                chains = 1, # How many chains to run
                iter = 10, # How many iterations per chain
                algorithm="NUTS")


test_fit_2 = stan(file = "Multi_LinReg.stan",
                data = stan_data_2,
                pars = params_monitor,
                chains = 1, # How many chains to run
                iter = 10, # How many iterations per chain
                algorithm="NUTS")

## Now we will run our full model:
# How many samples do we want of each parameter, from each chain?
n_mc_samples = 1000
# How much burn-in?
n_burn = 500
# How much thinning? (take the ith value of the chain)
n_thin = 3
# Total iterations needed:
n_iter_total = (n_mc_samples * n_thin) + n_burn

model_fit_1 =
 stan(fit = test_fit_1, # So it knows we're already compiled
      file = "Multi_LinReg.stan",
      data = stan_data_1,
      pars = params_monitor,
      chains = 3,
      warmup = n_burn,
      thin = n_thin,
      iter = n_iter_total,
      algorithm="NUTS")

model_fit_2 =
 stan(fit = test_fit_2, # So it knows we're already compiled
      file = "Multi_LinReg.stan",
      data = stan_data_2,
      pars = params_monitor,
      chains = 3,
      warmup = n_burn,
      thin = n_thin,
      iter = n_iter_total,
      algorithm="NUTS")

```
```{r, message=FALSE, warning=FALSE}
tail(summary(model_fit_1)$summary[, "Rhat"],1)
tail(summary(model_fit_2)$summary[, "Rhat"],1)
```
```{r}
#the median
summary(model_fit_1)$summary[1:3, "50%"]
summary(model_fit_2)$summary[1:3, "50%"]
```
```{r}
#the 95% credible intervals
summary(model_fit_1)$summary[1:3, "2.5%"]
summary(model_fit_1)$summary[1:3, "97.5%"]
```
```{r}
#the 95% credible intervals
summary(model_fit_2)$summary[1:3, "2.5%"]
summary(model_fit_2)$summary[1:3, "97.5%"]
```
2. Use the \texttt{loo}() function to calculate the LOO-IC for each model. Follow my example code below, and make sure to store these into separate objects for each model run.
```{r}

# Because eval = FALSE this code chunk will not run. 
# You will have to input your correct objects into this function.

# Calculate the LOO and WAIC for a single model:
# I'm doing this for the 'full' or most complex model
log_lik_full = extract_log_lik(model_fit)
log_lik_1 = extract_log_lik(model_fit_1)
log_lik_2 = extract_log_lik(model_fit_2)

loo_full = loo(log_lik_full)
loo_sub1 = loo(log_lik_1)
loo_sub2 = loo(log_lik_2)

waic_full = waic(log_lik_full)
waic_sub1 = waic(log_lik_1)
waic_sub2 = waic(log_lik_2)


loo_full
loo_sub1
loo_sub2



waic_full
waic_sub1
waic_sub2
```

3. Use the \texttt{loo::compare}() function to compare all three of your nested models using the LOO-CV. Your code will look something like:
```{r}

# Because eval = FALSE this code chunk will not run. 
# You will have to input your correct objects into this function.

loo::compare(loo_full, loo_sub1, loo_sub2)

```


4. Give a brief interpretation of your results of this statistical analysis and model comparison, citing the quantitative metrics as support for your conclusions. Remember your goal is to choose the most parsimonious model. 
According to the resukt shown above, we may could conclude that the best model is model_fit_2, which only have the 2nd input.The value of elpd_loo is -215.1. Comapre to the full model, it is simpler even the has the close elpd_loo value.

5. Show me visually and quantitatively that the variance of the posterior samples of $\beta_2$ from the most complex model is \textbf{larger} than the variance of the same estimate from the simpler model. The difference will be subtle, but you can imagine that if you had many input variables the effect would become compounded. 
```{r}
hist(extract(model_fit)$beta[,2], main = paste("Histogram"),xlab=expression(~beta))
```
```{r}
hist(extract(model_fit_2)$beta, main = paste("Histogram"),xlab=expression(~beta))
```


```{r}
var(extract(model_fit)$beta[,2])
```
```{r}
var(extract(model_fit_2)$beta)
```












