---
title: "Lab7"
author: "Jun Rao"
date: "10/8/2019"
output:
  word_document: default
  html_document: default
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h')
```

# Hierarchical linear regression

In this lab we will test our understanding of random effects (mixed-effects) modeling.

```{r message=FALSE, warning=FALSE}
#--------------------------------------
#--------------------------------------
# Import/Load the rstan library:
library(tidyverse)
library(rstan)
library(loo)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
#--------------------------------------
#--------------------------------------
## Simulating the data
```


## Simulating the data

Our first task is to simulate data following the assumptions of random-effects modeling. First, we will assume a true model in which groups have an equal slope, but vary in their intercept:
$$y_i = (\bar{\alpha} + \eta_{j,[i]}) + \beta x_{i}$$

# Task 1 (25 points)

Complete the following:

1. Add comments to this code-chunk to convey your understanding (2 points). 

```{r, fig.align='center', fig.height=4, fig.width=6}

#creating simulations or random objects that can be reproduced.
set.seed(7)

#We have 8 groups
n_group = 8

#how many data points in each group
n_samp_per_group = ceiling(runif(n_group, 4, 15))

group_idx = rep(c(1:n_group), times = n_samp_per_group)

#total data points in all groups
n_sample = sum(n_samp_per_group)

#Each Parameter in the model neea a prior
alpha_mean = rnorm(1, 0, 10)
alpha_sigma = abs(rnorm(1, 0, 10))
alpha_eta_vec = rnorm(n_group, 0, alpha_sigma)
  
beta = rnorm(1, 0, 10)

sigma_resid = abs(rnorm(1, 0, 2))

#input value
x_raw = runif(n_sample, 0, 3500)
x_mean = mean(x_raw)
x_sd = sd(x_raw)

#center and standardize:
x_scaled = (x_raw - x_mean) / (2 * x_sd)

epsilon = rnorm(n_sample, 0, sigma_resid)


y_hat = NULL
for(i in 1:n_sample){
  
  y_hat[i] = (alpha_mean + alpha_eta_vec[group_idx[i]]) + x_scaled[i] * beta
  
}

#Output Value
y_obs = y_hat + epsilon


data_df = 
  data.frame(y_obs, x_scaled, group_idx) %>%
  mutate(group_idx = factor(group_idx))

#draw the graphics for each group
ggplot(data_df, aes(x = x_scaled, y = y_obs, color = group_idx)) +
  geom_point(shape = 19) +
  geom_smooth(method = "lm", se = FALSE, size = 0.2) +
  scale_color_brewer(palette = "Set1") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_classic()


```

\newpage

## Fitting the Stan models

2. Use the supplied Stan code to fit the random effects model to your generated data. \textbf{Hint}: Remember to monitor your \texttt{log\_lik} variable so that you can complete sub-task 5, below (5 points). 

```{r, message=FALSE, warning=FALSE}
stan_data = list(n_sample = n_sample,
                 n_group = n_group,
                 y_vec = y_obs,
                 x_vec = x_scaled,
                 group_idx = group_idx)

params_monitor = c("beta", "alpha_mean", "eta_alpha", "alpha_sigma", "sigma_resid", "log_lik")

test_fit_1 = stan( file = "Intercepts_LinReg.stan",
                 data = stan_data,
                 pars = params_monitor,
                 chains = 1,
                 iter = 10,
                 algorithm="NUTS")
## Now we will run our full model:
# How many samples do we want of each parameter, from each chain?
n_mc_samples = 1000
# How much burn-in?
n_burn = 500
# How much thinning? (take the ith value of the chain)
n_thin = 3
# Total iterations needed:
n_iter_total = (n_mc_samples * n_thin) + n_burn
model_fit_1 =
stan(fit = test_fit_1, # So it knows we're already compiled
     file = "Intercepts_LinReg.stan",
     data = stan_data,
     pars = params_monitor,
     chains = 3,
     warmup = n_burn,
     thin = n_thin,
     iter = n_iter_total,
     algorithm="NUTS")
model_out_1 = rstan::extract(model_fit_1)
str(model_out_1)
```

3. Construct at least two visualizations to validate that your model was able to adequately recover the true parameters. Note: A table can be considered a visualization, because it organizes the quantitative information in a visually accessible manner (5 points). 

```{r}
#first we visualize tabularly, the 95% CI for estimated parameters and compare these the the true values
model_sum = summary(model_fit_1)$summary
credint = model_sum[c(1:12),c(4,6,8)]
credint2 = cbind(credint, c(beta, alpha_mean, alpha_sigma, alpha_eta_vec[1:8],sigma_resid))
colnames(credint2)[4] <- "true_vals" 

# correct true eta values
corrected = c(beta, alpha_mean + mean(alpha_eta_vec), alpha_sigma, alpha_eta_vec + alpha_mean, sigma_resid)
credint2 = cbind(credint2, corrected)
colnames(credint2)[5] <- "corrected"
credint2
# we can see that few parameters were adequately recoverd. This included alpha_mean and all of the eta_alphas (9 of 12 params)

```
```{r example-plot2, fig.height=10, fig.width=10, fig.align='center'}
par(mfrow = c(5,3))
hist(model_out_1$beta,main="the marginal posterior samples of beta",xlab = expression(beta))
abline(v = beta, lty = 1, lwd = 2, col = "blue")

hist(model_out_1$alpha_mean,main="the marginal posterior samples of alpha_mean",xlab = expression(alpha_mean))
abline(v = alpha_mean, lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,1],main="the marginal posterior samples of eta_alpha[1]",xlab = expression(eta_alpha[1]))
abline(v = alpha_eta_vec[1], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,2],main="the marginal posterior samples of eta_alpha[2]",xlab = expression(eta_alpha[2]))
abline(v = alpha_eta_vec[2], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,3],main="the marginal posterior samples of eta_alpha[3]",xlab = expression(eta_alpha[3]))
abline(v = alpha_eta_vec[3], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,4],main="the marginal posterior samples of eta_alpha[4]",xlab = expression(eta_alpha[4]))
abline(v = alpha_eta_vec[4], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,5],main="the marginal posterior samples of eta_alpha[5]",xlab = expression(eta_alpha[5]))
abline(v = alpha_eta_vec[5], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,6],main="the marginal posterior samples of eta_alpha[6]",xlab = expression(eta_alpha[6]))
abline(v = alpha_eta_vec[6], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,7],main="the marginal posterior samples of eta_alpha[7]",xlab = expression(eta_alpha[7]))
abline(v = alpha_eta_vec[7], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$eta_alpha[,8],main="the marginal posterior samples of eta_alpha[8]",xlab = expression(eta_alpha[8]))
abline(v = alpha_eta_vec[8], lty = 1, lwd = 2, col = "blue")

hist(model_out_1$beta,main="the marginal posterior samples of beta",xlab = expression(beta))
abline(v = beta, lty = 1, lwd = 2, col = "blue")

hist(model_out_1$alpha_sigma,main="the marginal posterior samples of alpha_sigma",xlab = expression(alpha_sigma))
abline(v = alpha_sigma, lty = 1, lwd = 2, col = "blue")

hist(model_out_1$sigma_resid,main="the marginal posterior samples of sigma_resid",xlab = expression(sigma_resid))
abline(v = sigma_resid, lty = 1, lwd = 2, col = "blue")


```

4. Construct a Stan code file that allows for both random intercepts and random slopes. Fit this new, more complex model to your same data set (8 points). 


```{r, message=FALSE, warning=FALSE}
beta_mean = rnorm(1, 0, 10)
beta_sigma = abs(rnorm(1, 0, 10))
beta_eta_vec = rnorm(n_group, 0, alpha_sigma)

params_monitor = c("beta_mean", "alpha_mean", "eta_beta", "eta_alpha", "beta_sigma", "alpha_sigma", "sigma_resid", "log_lik")

test_fit_2 = stan( file = "Lab7.stan",
                 data = stan_data,
                 pars = params_monitor,
                 chains = 1,
                 iter = 10,
                 algorithm="NUTS")
## Now we will run our full model:
# How many samples do we want of each parameter, from each chain?
n_mc_samples = 1000
# How much burn-in?
n_burn = 500
# How much thinning? (take the ith value of the chain)
n_thin = 3
# Total iterations needed:
n_iter_total = (n_mc_samples * n_thin) + n_burn
model_fit_2 =
stan(fit = test_fit_2, # So it knows we're already compiled
     file = "Intercepts_LinReg.stan",
     data = stan_data,
     pars = params_monitor,
     chains = 3,
     warmup = n_burn,
     thin = n_thin,
     iter = n_iter_total,
     algorithm="NUTS")
model_out_2 = rstan::extract(model_fit_2)
str(model_out_2)

```
```{r, message=FALSE, warning=FALSE}

#summary(model_fit_2)$summary
#first we visualize tabularly, the 95% CI for estimated parameters and compare these the the true values
model_sum = summary(model_fit_2)$summary
credint = model_sum[c(1:21),c(4,6,8)]
credint2 = cbind(credint, c(beta_mean, alpha_mean, beta_sigma, alpha_sigma, beta_eta_vec[1:8], alpha_eta_vec[1:8],sigma_resid))
colnames(credint2)[4] <- "true_vals" 

# correct true eta values
corrected = c(beta_mean+mean(beta_eta_vec), alpha_mean + mean(alpha_eta_vec), alpha_sigma, beta_eta_vec + beta_mean, alpha_eta_vec + alpha_mean, sigma_resid)
credint2 = cbind(credint2, corrected)
colnames(credint2)[5] <- "corrected"
credint2
# we can see that few parameters were adequately recoverd. This included alpha_mean and all of the eta_alphas (9 of 12 params)
```

5. Calculate the LOO-IC for each of your two models. Comment on which model is more parsimonious, given the data at hand (5 points). 
```{r, message=FALSE, warning=FALSE}
# Calculate the LOO and WAIC for a single model:
# Full model
log_lik_1 = extract_log_lik(model_fit_1) 
loo_1 = loo(log_lik_1)
loo_1$estimates
# reduced, intercept RE model
log_lik_2 = extract_log_lik(model_fit_2) 
loo_2 = loo(log_lik_2)
loo_2$estimates
# model comparison of loo
loo::compare(loo_1, loo_2)
```



# Task 2 (15 points)

1. Subset your data to include only one of the groups. 

```{r}
# choosing Group 1 as the single group for this task
data_group1 = data_df[data_df$group_idx == 1, ]
```
2. For that specific group, create a scatterplot. 

(For the following, use your most parsimonious model)

```{r}
ggplot(data_group1, aes(x = x_scaled, y = y_obs)) + 
  geom_point() +
  labs(title = 'Group 1',
       x = expression(X[scaled]),
       y = expression(Y)) +
  geom_smooth(method = "lm", se = FALSE, size = 0.2)

```




3. On the scatterplot, overlay the median model-fit (i.e., the median estimated line). 
```{r}
group_idx_rd = sample(c(1:n_group),1)
data_df_sub = data_df[group_idx == group_idx_rd,]
median_set = summary(model_fit_1)$summary[1:12, '50%']

beta_median = median_set[1]
alpha_mean_median = median_set[2]
eta_alpha_media = median_set[2 + group_idx_rd]


```




4. Bootstrap from the posterior to overlay many more possible model-fits. Be sure that for each of these model-fits, you are drawing \textit{joint} parameter sets from your MCMC samples. \textbf{Hint}: To get a joint draw, first specify which sample you will take from your MCMC chains. For instance:
```{r eval=FALSE}
plot(data_df[,2],data_df[,1],pch=16,xlab="X",ylab="y")
for (i in 1:50) {
  temp_idx = sample(c(1:n_iter_total-n_burn), 1)
   temp_alpha_mean = model_out_1$alpha_mean[temp_idx]
   temp_eta_alph = model_out_1$eta_alpha[temp_idx,group_idx_rd]
   temp_beta = model_out_1$beta[temp_idx]
   abline(a = temp_eta_alph + temp_alpha_mean, b = temp_beta, col = scales::alpha("gray",alpha=0.5)) 
}

abline(a = alpha_mean_median + eta_alpha_media, b= beta_median,col="black")
```

5. Make sure it is easy to visually distinguish between the median model-fit and the other possible model-fits. (You might want to plot the ``other'' fits before overlaying the median fit). 


